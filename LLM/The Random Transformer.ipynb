{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2a5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d285e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5756b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.  , 8.  , 4.  ],\n",
       "       [6.84, 9.99, 6.84]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = np.array([[1, 3, 3, 5], [2.84, 3.99, 4, 6]])\n",
    "K1 = embedding @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223299ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.  , 6.  , 4.  ],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = embedding @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a7c7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.  , 3.  , 3.  ],\n",
       "       [9.99, 3.99, 4.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = embedding @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44ab523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.    , 105.21  ],\n",
       "       [ 87.88  , 135.5517]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c283037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.2598183 , 60.74302182],\n",
       "       [50.73754166, 78.26081048]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(3)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e89eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.67695573e-10, 1.00000000e+00],\n",
       "       [1.11377182e-12, 1.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = np.array(x)  # Convert input to numpy array\n",
    "    if len(x.shape) == 1:  # Handle 1D arrays by adding a new axis\n",
    "        x = x[np.newaxis, :]\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccbe6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664afe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(3)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a676e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(embedding, WQ1, WK1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e819d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84, 3.99, 7.99],\n",
       "       [8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(embedding, WQ2, WK2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd9b56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84, 8.84, 3.99, 7.99],\n",
       "       [7.99, 8.84, 6.84, 8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5caaa8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.95481735, -14.12627891, -12.49250332, -18.50804518],\n",
       "       [ 11.95481735, -14.12627891, -12.49250332, -18.50804518]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just some random values\n",
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "Z = attentions @ W\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb537ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8020ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 39.23832613,  10.94504339, -10.11288266, -31.57712331],\n",
       "       [ 39.23832613,  10.94504339, -10.11288266, -31.57712332]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a4352bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "\n",
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "def multi_head_attention(x, WQs, WKs, WVs):\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return Z\n",
    "\n",
    "def random_encoder_block(x):\n",
    "    WQs = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "748d4283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 3.  , 3.  , 5.  ],\n",
       "       [2.84, 3.99, 4.  , 6.  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f09468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.30544367, 55.35720351, -8.94552101, 36.31134929],\n",
       "       [ 6.30144709, 55.30167828, -8.92573111, 36.25953458]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "683eb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40352854.00023742, 36941199.2395912 , -4769993.4050151 ,\n",
       "        -3355840.30156874],\n",
       "       [40352854.00023742, 36941199.2395912 , -4769993.4050151 ,\n",
       "        -3355840.30156874]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb21b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.29300251],\n",
       "       [-4.08550252]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).mean(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9620cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.63170702],\n",
       "       [10.99362604]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).std(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74843210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26cc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71635826, -0.54866785, -0.39499776, -0.77269265],\n",
       "       [ 1.7173877 , -0.55038945, -0.40086868, -0.76612956]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(Z + embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "276d1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.54019984, -0.65334771,  0.19205135, -1.07890348],\n",
       "       [ 1.54019984, -0.65334771,  0.19205135, -1.07890348]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab1dd1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.68583465,  8.65844136, -3.28747964, -0.78329174]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_embedding = 4\n",
    "n_attention_heads = 2\n",
    "\n",
    "E = np.array([[1, 1, 0, 1]])\n",
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "Z_self_attention = multi_head_attention(E, WQs, WKs, WVs)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "949d3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92256896,  1.64143633, -0.67261632, -0.04625105]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_self_attention = layer_norm(Z_self_attention + E)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf92f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    # The next three lines are the key difference!\n",
    "    K = encoder_output @ WK    # Note that now we pass the previous encoder output!\n",
    "    V = encoder_output @ WV    # Note that now we pass the previous encoder output!\n",
    "    Q = attention_input @ WQ   # Same as self-attention\n",
    "\n",
    "    # This stays the same\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_encoder_decoder_attention(\n",
    "    encoder_output, attention_input, WQs, WKs, WVs\n",
    "):\n",
    "    # Note that now we pass the previous encoder output!\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(\n",
    "                encoder_output, attention_input, WQ, WK, WV\n",
    "            )\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf619354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.00573814, 12.60121576, -9.62652818, -5.07636579]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "    encoder_output, Z_self_attention, WQs, WKs, WVs\n",
    ")\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c55026b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20164373,  1.5218806 , -1.14278738, -0.58073694]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z_self_attention)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0439251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57909149,  1.71950627, -0.74002355, -0.40039123]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)\n",
    "\n",
    "output = layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) + Z_encoder_decoder)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15a6b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "def decoder_block(\n",
    "    x,\n",
    "    encoder_output,\n",
    "    WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "    W1, b1, W2, b2,\n",
    "):\n",
    "    # Same as before\n",
    "    Z = multi_head_attention(\n",
    "        x, WQs_self_attention, WKs_self_attention, WVs_self_attention\n",
    "    )\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    # The next three lines are the key difference!\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention\n",
    "    )\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    # Same as before\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)\n",
    "\n",
    "def random_decoder_block(x, encoder_output):\n",
    "    # Just a bunch of random initializations\n",
    "    WQs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    WQs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "\n",
    "    return decoder_block(\n",
    "        x, encoder_output,\n",
    "        WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "        W1, b1, W2, b2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db2cf11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38703139, -0.34892723,  0.39489996,  1.34105865]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder(x, decoder_embedding, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_decoder_block(x, decoder_embedding)\n",
    "    return x\n",
    "\n",
    "decoder(E, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56fc311d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.67179275,  0.17198718, -1.69655828,  1.33603561,  0.86000803,\n",
       "        2.43942626, -0.30797465,  0.77796867,  1.541172  ,  1.50069719])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b\n",
    "\n",
    "x = linear([1, 0, 1, 0], np.random.randn(4, 10), np.random.randn(10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ded80cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01617342, 0.03760545, 0.00580433, 0.1204455 , 0.07482633,\n",
       "        0.36306722, 0.02327051, 0.06893268, 0.14787003, 0.14200453]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "542a0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': array([-1.06998789, -0.38733348, -0.30939307, -0.45660257]),\n",
       " 'mundo': array([-0.76956495,  1.94292748,  1.05118722, -0.35117607]),\n",
       " 'world': array([-1.0570816 , -0.00895372, -0.0984772 ,  0.08861666]),\n",
       " 'how': array([ 0.08524699, -0.48298981,  0.26719934,  0.08619517]),\n",
       " '?': array([-0.05613469,  0.25952293, -0.14341733,  0.19177927]),\n",
       " 'EOS': array([ 0.24207383, -0.5176897 , -0.53633206, -0.6855262 ]),\n",
       " 'SOS': array([-0.39902152, -1.95851238, -0.64138367,  0.92162645]),\n",
       " 'a': array([ 0.60303594, -0.48169978,  0.50561151, -0.53957713]),\n",
       " 'hola': array([ 0.68195484, -1.90854728, -0.57311108, -0.16569104]),\n",
       " 'c': array([-2.18418514,  1.78115469, -0.90508235, -1.39450771])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\n",
    "    \"hello\",\n",
    "    \"mundo\",\n",
    "    \"world\",\n",
    "    \"how\",\n",
    "    \"?\",\n",
    "    \"EOS\",\n",
    "    \"SOS\",\n",
    "    \"a\",\n",
    "    \"hola\",\n",
    "    \"c\",\n",
    "]\n",
    "embedding_reps = np.random.randn(10, 4)\n",
    "vocabulary_embeddings = {\n",
    "    word: embedding_reps[i] for i, word in enumerate(vocabulary)\n",
    "}\n",
    "vocabulary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88522c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_sequence, max_iters=3):\n",
    "    # We first encode the inputs into embeddings\n",
    "    # This skips the positional encoding step for simplicity\n",
    "    embedded_inputs = [\n",
    "        vocabulary_embeddings[token] for token in input_sequence\n",
    "    ]\n",
    "    print(\"Embedding representation (encoder input)\", embedded_inputs)\n",
    "\n",
    "    # We then generate an embedding representation\n",
    "    encoder_output = encoder(embedded_inputs)\n",
    "    print(\"Embedding generated by encoder (encoder output)\", encoder_output)\n",
    "\n",
    "    # We initialize the decoder output with the embedding of the start token\n",
    "    sequence_embeddings = [vocabulary_embeddings[\"SOS\"]]\n",
    "    output = \"SOS\"\n",
    "    \n",
    "    # Random matrices for the linear layer\n",
    "    W_linear = np.random.randn(d_embedding, len(vocabulary))\n",
    "    b_linear = np.random.randn(len(vocabulary))\n",
    "\n",
    "    # We limit number of decoding steps to avoid too long sequences without EOS\n",
    "    for i in range(max_iters):\n",
    "        # Decoder step\n",
    "        decoder_output = decoder(sequence_embeddings, encoder_output)\n",
    "\n",
    "        # Only use the last output for prediction\n",
    "        logits = linear(decoder_output[-1], W_linear, b_linear)\n",
    "        # We wrap logits in a list as our softmax expects batches/2D array\n",
    "        probs = softmax([logits])\n",
    "\n",
    "        # We get the most likely next token\n",
    "        next_token = vocabulary[np.argmax(probs)]\n",
    "        sequence_embeddings.append(vocabulary_embeddings[next_token])\n",
    "        output += \" \" + next_token\n",
    "\n",
    "        print(\n",
    "            \"Iteration\", i, \n",
    "            \"next token\", next_token,\n",
    "            \"with probability of\", np.max(probs),\n",
    "        )\n",
    "\n",
    "        # If the next token is the end token, we return the sequence\n",
    "        if next_token == \"EOS\":\n",
    "            return output\n",
    "\n",
    "    return output, sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec8d34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding representation (encoder input) [array([-1.06998789, -0.38733348, -0.30939307, -0.45660257]), array([-1.0570816 , -0.00895372, -0.0984772 ,  0.08861666])]\n",
      "Embedding generated by encoder (encoder output) [[ 1.39061441  0.49642812 -1.08367269 -0.80336984]\n",
      " [ 1.3906145   0.49642797 -1.08367269 -0.80336978]]\n",
      "Iteration 0 next token how with probability of 0.4213570599466652\n",
      "Iteration 1 next token how with probability of 0.8761176187808272\n",
      "Iteration 2 next token SOS with probability of 0.4491314403338158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SOS how how SOS',\n",
       " [array([-0.39902152, -1.95851238, -0.64138367,  0.92162645]),\n",
       "  array([ 0.08524699, -0.48298981,  0.26719934,  0.08619517]),\n",
       "  array([ 0.08524699, -0.48298981,  0.26719934,  0.08619517]),\n",
       "  array([-0.39902152, -1.95851238, -0.64138367,  0.92162645])])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate([\"hello\", \"world\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
