{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2a5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d285e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5756b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.  , 8.  , 4.  ],\n",
       "       [6.84, 9.99, 6.84]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = np.array([[1, 3, 3, 5], [2.84, 3.99, 4, 6]])\n",
    "K1 = embedding @ WK1\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223299ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.  , 6.  , 4.  ],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = embedding @ WV1\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a7c7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.  , 3.  , 3.  ],\n",
       "       [9.99, 3.99, 4.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = embedding @ WQ1\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44ab523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.    , 105.21  ],\n",
       "       [ 87.88  , 135.5517]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c283037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.2598183 , 60.74302182],\n",
       "       [50.73754166, 78.26081048]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = scores1 / np.sqrt(3)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e89eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.67695573e-10, 1.00000000e+00],\n",
       "       [1.11377182e-12, 1.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = np.array(x)  # Convert input to numpy array\n",
    "    if len(x.shape) == 1:  # Handle 1D arrays by adding a new axis\n",
    "        x = x[np.newaxis, :]\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccbe6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = scores1 @ V1\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664afe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(3)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a676e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84],\n",
       "       [7.99, 8.84, 6.84]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention1 = attention(embedding, WQ1, WK1, WV1)\n",
    "attention1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e819d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84, 3.99, 7.99],\n",
       "       [8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention2 = attention(embedding, WQ2, WK2, WV2)\n",
    "attention2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd9b56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.99, 8.84, 6.84, 8.84, 3.99, 7.99],\n",
       "       [7.99, 8.84, 6.84, 8.84, 3.99, 7.99]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = np.concatenate([attention1, attention2], axis=1)\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5caaa8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.95481735, -14.12627891, -12.49250332, -18.50804518],\n",
       "       [ 11.95481735, -14.12627891, -12.49250332, -18.50804518]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just some random values\n",
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "Z = attentions @ W\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb537ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac56f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8020ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.96671815,  33.63661787, -36.76354977, -38.03381201],\n",
       "       [  4.96671815,  33.63661787, -36.76354978, -38.03381202]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = feed_forward(Z, W1, b1, W2, b2)\n",
    "output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a4352bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "\n",
    "def attention(x, WQ, WK, WV):\n",
    "    K = x @ WK\n",
    "    V = x @ WV\n",
    "    Q = x @ WQ\n",
    "\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "def multi_head_attention(x, WQs, WKs, WVs):\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "def feed_forward(Z, W1, b1, W2, b2):\n",
    "    return relu(Z.dot(W1) + b1).dot(W2) + b2\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return Z\n",
    "\n",
    "def random_encoder_block(x):\n",
    "    WQs = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "748d4283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 3.  , 3.  , 5.  ],\n",
       "       [2.84, 3.99, 4.  , 6.  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f09468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  89.93068206,    1.11549344,  -40.16761922, -105.61800212],\n",
       "       [  87.32358868,    2.66121053,  -39.94791518, -104.3511874 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "683eb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   822335.58296085, -20651612.30220575, -16447459.15878337,\n",
       "         -7748784.18792568],\n",
       "       [   822335.58296085, -20651612.30220575, -16447459.15878337,\n",
       "         -7748784.18792568]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb21b3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.29300251],\n",
       "       [-4.08550252]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).mean(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9620cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.63170702],\n",
       "       [10.99362604]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding + Z).std(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74843210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "def encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n",
    "    Z = multi_head_attention(x, WQs, WKs, WVs)\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    output = feed_forward(Z, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26cc396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71635826, -0.54866785, -0.39499776, -0.77269265],\n",
       "       [ 1.7173877 , -0.55038945, -0.40086868, -0.76612956]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm(Z + embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "276d1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.06393141,  0.14680229,  1.55991141, -0.64278229],\n",
       "       [-1.06391719,  0.14678596,  1.55991998, -0.64278875]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab1dd1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.95866806,  3.3584017 , -8.72447368,  5.78033371]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_embedding = 4\n",
    "n_attention_heads = 2\n",
    "\n",
    "E = np.array([[1, 1, 0, 1]])\n",
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "Z_self_attention = multi_head_attention(E, WQs, WKs, WVs)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "949d3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80538571,  0.26496303, -1.69885904,  0.6285103 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_self_attention = layer_norm(Z_self_attention + E)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf92f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n",
    "    # The next three lines are the key difference!\n",
    "    K = encoder_output @ WK    # Note that now we pass the previous encoder output!\n",
    "    V = encoder_output @ WV    # Note that now we pass the previous encoder output!\n",
    "    Q = attention_input @ WQ   # Same as self-attention\n",
    "\n",
    "    # This stays the same\n",
    "    scores = Q @ K.T\n",
    "    scores = scores / np.sqrt(d_key)\n",
    "    scores = softmax(scores)\n",
    "    scores = scores @ V\n",
    "    return scores\n",
    "\n",
    "\n",
    "def multi_head_encoder_decoder_attention(\n",
    "    encoder_output, attention_input, WQs, WKs, WVs\n",
    "):\n",
    "    # Note that now we pass the previous encoder output!\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(\n",
    "                encoder_output, attention_input, WQ, WK, WV\n",
    "            )\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf619354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.33900838,  1.19704191,  2.27804123, -0.26290895]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "    encoder_output, Z_self_attention, WQs, WKs, WVs\n",
    ")\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c55026b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.70528006,  0.84955343,  0.47336843,  0.3823582 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z_self_attention)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0439251d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5219184 , -1.73048459,  0.64106057,  0.56750562]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)\n",
    "\n",
    "output = layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) + Z_encoder_decoder)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15a6b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 3\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n",
    "\n",
    "def decoder_block(\n",
    "    x,\n",
    "    encoder_output,\n",
    "    WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "    W1, b1, W2, b2,\n",
    "):\n",
    "    # Same as before\n",
    "    Z = multi_head_attention(\n",
    "        x, WQs_self_attention, WKs_self_attention, WVs_self_attention\n",
    "    )\n",
    "    Z = layer_norm(Z + x)\n",
    "\n",
    "    # The next three lines are the key difference!\n",
    "    Z_encoder_decoder = multi_head_encoder_decoder_attention(\n",
    "        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention\n",
    "    )\n",
    "    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n",
    "\n",
    "    # Same as before\n",
    "    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n",
    "    return layer_norm(output + Z_encoder_decoder)\n",
    "\n",
    "def random_decoder_block(x, encoder_output):\n",
    "    # Just a bunch of random initializations\n",
    "    WQs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    WQs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "\n",
    "    return decoder_block(\n",
    "        x, encoder_output,\n",
    "        WQs_self_attention, WKs_self_attention, WVs_self_attention,\n",
    "        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n",
    "        W1, b1, W2, b2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db2cf11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86819809,  0.05077623,  0.72823997, -1.6472143 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decoder(x, decoder_embedding, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_decoder_block(x, decoder_embedding)\n",
    "    return x\n",
    "\n",
    "decoder(E, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56fc311d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.08011155,  0.41452874,  0.29163508,  2.18390481,  0.56179814,\n",
       "       -2.77183395, -0.18728854, -0.32215983, -1.92614305, -1.87526253])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear(x, W, b):\n",
    "    return np.dot(x, W) + b\n",
    "\n",
    "x = linear([1, 0, 1, 0], np.random.randn(4, 10), np.random.randn(10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ded80cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16051384, 0.08249992, 0.07295944, 0.48404284, 0.09558986,\n",
       "        0.00340905, 0.04519471, 0.03949242, 0.00794169, 0.00835622]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "542a0843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': array([-0.82087961, -0.64054363, -0.38639977, -1.00798525]),\n",
       " 'mundo': array([-0.60512693, -0.1352113 ,  0.90482958, -1.56891667]),\n",
       " 'world': array([ 1.14470487, -1.85164297,  0.92776147, -0.51977999]),\n",
       " 'how': array([ 0.62038366,  0.89215519,  0.81909244, -0.02642335]),\n",
       " '?': array([ 0.63584655,  1.39137262, -1.42092484, -0.1498072 ]),\n",
       " 'EOS': array([-1.4973367 , -0.60157315, -1.18544969,  1.37872054]),\n",
       " 'SOS': array([-0.10597324, -0.55418694,  0.13619576,  0.93146755]),\n",
       " 'a': array([0.14341067, 0.32762159, 0.03070575, 0.55885494]),\n",
       " 'hola': array([ 0.51286258, -0.46369143, -1.56593688,  0.38688579]),\n",
       " 'c': array([-0.53763329,  0.47407343,  0.44130252, -0.11569071])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\n",
    "    \"hello\",\n",
    "    \"mundo\",\n",
    "    \"world\",\n",
    "    \"how\",\n",
    "    \"?\",\n",
    "    \"EOS\",\n",
    "    \"SOS\",\n",
    "    \"a\",\n",
    "    \"hola\",\n",
    "    \"c\",\n",
    "]\n",
    "embedding_reps = np.random.randn(10, 4)\n",
    "vocabulary_embeddings = {\n",
    "    word: embedding_reps[i] for i, word in enumerate(vocabulary)\n",
    "}\n",
    "vocabulary_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88522c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_sequence, max_iters=3):\n",
    "    # We first encode the inputs into embeddings\n",
    "    # This skips the positional encoding step for simplicity\n",
    "    embedded_inputs = [\n",
    "        vocabulary_embeddings[token] for token in input_sequence\n",
    "    ]\n",
    "    print(\"Embedding representation (encoder input)\", embedded_inputs)\n",
    "\n",
    "    # We then generate an embedding representation\n",
    "    encoder_output = encoder(embedded_inputs)\n",
    "    print(\"Embedding generated by encoder (encoder output)\", encoder_output)\n",
    "\n",
    "    # We initialize the decoder output with the embedding of the start token\n",
    "    sequence_embeddings = [vocabulary_embeddings[\"SOS\"]]\n",
    "    output = \"SOS\"\n",
    "    \n",
    "    # Random matrices for the linear layer\n",
    "    W_linear = np.random.randn(d_embedding, len(vocabulary))\n",
    "    b_linear = np.random.randn(len(vocabulary))\n",
    "\n",
    "    # We limit number of decoding steps to avoid too long sequences without EOS\n",
    "    for i in range(max_iters):\n",
    "        # Decoder step\n",
    "        decoder_output = decoder(sequence_embeddings, encoder_output)\n",
    "\n",
    "        # Only use the last output for prediction\n",
    "        logits = linear(decoder_output[-1], W_linear, b_linear)\n",
    "        # We wrap logits in a list as our softmax expects batches/2D array\n",
    "        probs = softmax([logits])\n",
    "\n",
    "        # We get the most likely next token\n",
    "        next_token = vocabulary[np.argmax(probs)]\n",
    "        sequence_embeddings.append(vocabulary_embeddings[next_token])\n",
    "        output += \" \" + next_token\n",
    "\n",
    "        print(\n",
    "            \"Iteration\", i, \n",
    "            \"next token\", next_token,\n",
    "            \"with probability of\", np.max(probs),\n",
    "        )\n",
    "\n",
    "        # If the next token is the end token, we return the sequence\n",
    "        if next_token == \"EOS\":\n",
    "            return output\n",
    "\n",
    "    return output, sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec8d34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding representation (encoder input) [array([-0.82087961, -0.64054363, -0.38639977, -1.00798525]), array([ 1.14470487, -1.85164297,  0.92776147, -0.51977999])]\n",
      "Embedding generated by encoder (encoder output) [[-0.57053348 -0.56023307  1.73185505 -0.60108849]\n",
      " [-0.57053347 -0.5602332   1.73185505 -0.60108838]]\n",
      "Iteration 0 next token hola with probability of 0.4517065200675811\n",
      "Iteration 1 next token hola with probability of 0.35317224393434704\n",
      "Iteration 2 next token c with probability of 0.833188985883351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SOS hola hola c',\n",
       " [array([-0.10597324, -0.55418694,  0.13619576,  0.93146755]),\n",
       "  array([ 0.51286258, -0.46369143, -1.56593688,  0.38688579]),\n",
       "  array([ 0.51286258, -0.46369143, -1.56593688,  0.38688579]),\n",
       "  array([-0.53763329,  0.47407343,  0.44130252, -0.11569071])])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate([\"hello\", \"world\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
