{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e01fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = plt.imread(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "class AdvancedImageClassifier:\n",
    "    def __init__(self, input_shape=(224, 224, 3), num_classes=None, random_state=42):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.histories = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def preprocess_data(self, image_paths, labels, test_size=0.2, validation_split=0.2):\n",
    "        le = LabelEncoder()\n",
    "        labels_encoded = le.fit_transform(labels)\n",
    "        self.num_classes = len(le.classes_)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(image_paths, labels_encoded, test_size=test_size, \n",
    "                                                            random_state=self.random_state, stratify=labels_encoded)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split, \n",
    "                                                          random_state=self.random_state, stratify=y_train)\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def create_data_loaders(self, data, batch_size=32, num_workers=4, model_type='classification'):\n",
    "        if model_type == 'yolo':  # Custom YOLOv5 augmentation strategy\n",
    "            train_transform = A.Compose([\n",
    "                A.RandomResizedCrop(height=self.input_shape[0], width=self.input_shape[1]),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.3),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "                A.OneOf([\n",
    "                    A.OpticalDistortion(p=0.3),\n",
    "                    A.GridDistortion(p=0.2),\n",
    "                    A.PiecewiseAffine(p=0.3),\n",
    "                ], p=0.3),\n",
    "                A.OneOf([\n",
    "                    A.GaussianBlur(p=0.2),\n",
    "                    A.MotionBlur(p=0.2),\n",
    "                ], p=0.2),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:  # Standard classification augmentations\n",
    "            train_transform = A.Compose([\n",
    "                A.RandomResizedCrop(height=self.input_shape[0], width=self.input_shape[1]),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.2),\n",
    "                A.OneOf([\n",
    "                    A.OpticalDistortion(p=0.3),\n",
    "                    A.GridDistortion(p=0.1),\n",
    "                    A.PiecewiseAffine(p=0.3),\n",
    "                ], p=0.2),\n",
    "                A.OneOf([\n",
    "                    A.GaussNoise(p=0.2),\n",
    "                    A.GaussianBlur(p=0.2),\n",
    "                    A.MotionBlur(p=0.2),\n",
    "                ], p=0.2),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "        val_test_transform = A.Compose([\n",
    "            A.Resize(height=self.input_shape[0], width=self.input_shape[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "        train_dataset = ImageDataset(data['train']['image_paths'], data['train']['labels'], transform=train_transform)\n",
    "        val_dataset = ImageDataset(data['val']['image_paths'], data['val']['labels'], transform=val_test_transform)\n",
    "        test_dataset = ImageDataset(data['test']['image_paths'], data['test']['labels'], transform=val_test_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def create_model(self, model_name, pretrained=True):\n",
    "        if model_name == 'yolov10':\n",
    "            # Here we load YOLOv10 model\n",
    "            print(\"Loading YOLOv10 model, featuring NMS-free training and an efficiency-accuracy driven design.\")\n",
    "            model = YOLO('yolov10.pt')  # Replace with correct model path once available\n",
    "            model.to(self.device)\n",
    "        else:\n",
    "            # Load classification models\n",
    "            model = timm.create_model(model_name, pretrained=pretrained, num_classes=self.num_classes)\n",
    "            model = model.to(self.device)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, train_loader, val_loader, epochs=50, learning_rate=0.001):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f'best_{model.__class__.__name__}.pth')\n",
    "\n",
    "        return model, {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "\n",
    "    def evaluate_model(self, model, test_loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(self, train_loader, val_loader, test_loader, models_to_train):\n",
    "        for model_name in models_to_train:\n",
    "            print(f\"Training and evaluating {model_name}...\")\n",
    "            model = self.create_model(model_name)\n",
    "            model, history = self.train_model(model, train_loader, val_loader)\n",
    "            \n",
    "            self.models[model_name] = model\n",
    "            self.histories[model_name] = history\n",
    "            \n",
    "            results = self.evaluate_model(model, test_loader)\n",
    "            self.results[model_name] = results\n",
    "            \n",
    "            if results['accuracy'] > self.best_score:\n",
    "                self.best_score = results['accuracy']\n",
    "                self.best_model = model\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def plot_training_history(self, model_name):\n",
    "        history = self.histories[model_name]\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} - Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_results(self):\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "        results_df = pd.DataFrame({model: [self.results[model][metric] for metric in metrics] \n",
    "                                   for model in self.results.keys()}, index=metrics)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(results_df, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "        plt.title('Model Comparison')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, model_name):\n",
    "        cm = self.results[model_name]['confusion_matrix']\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "\n",
    "def load_image_data(data_source, split=True, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    if isinstance(data_source, str):\n",
    "        if os.path.isfile(data_source) and data_source.endswith('.csv'):\n",
    "            # Load from CSV file\n",
    "            df = pd.read_csv(data_source)\n",
    "            image_paths = df['image_path'].tolist()\n",
    "            labels = df['label'].tolist()\n",
    "        elif os.path.isdir(data_source):\n",
    "            # Load from directory structure\n",
    "            image_paths = []\n",
    "            labels = []\n",
    "            for class_name in os.listdir(data_source):\n",
    "                class_dir = os.path.join(data_source, class_name)\n",
    "                if os.path.isdir(class_dir):\n",
    "                    for image_name in os.listdir(class_dir):\n",
    "                        image_path = os.path.join(class_dir, image_name)\n",
    "                        image_paths.append(image_path)\n",
    "                        labels.append(class_name)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid data source. Please provide a CSV file or a directory.\")\n",
    "    elif isinstance(data_source, pd.DataFrame):\n",
    "        # Data is already in a DataFrame\n",
    "        image_paths = data_source['image_path'].tolist()\n",
    "        labels = data_source['label'].tolist()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data source. Please provide a string path or a pandas DataFrame.\")\n",
    "\n",
    "    if split:\n",
    "        # Split the data into train+val and test sets\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            image_paths, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "        )\n",
    "        \n",
    "        # Split the train+val set into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=val_size, random_state=random_state, stratify=y_train_val\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': {'image_paths': X_train, 'labels': y_train},\n",
    "            'val': {'image_paths': X_val, 'labels': y_val},\n",
    "            'test': {'image_paths': X_test, 'labels': y_test}\n",
    "        }\n",
    "    else:\n",
    "        return {'image_paths': image_paths, 'labels': labels}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your image data here\n",
    "    # image_paths should be a list of file paths to your images\n",
    "    # labels should be a list of corresponding labels\n",
    "    # image_paths, labels = load_image_data()\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    clf = AdvancedImageClassifier(input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = clf.preprocess_data(image_paths, labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = clf.create_data_loaders({\n",
    "        'train': {'image_paths': X_train, 'labels': y_train},\n",
    "        'val': {'image_paths': X_val, 'labels': y_val},\n",
    "        'test': {'image_paths': X_test, 'labels': y_test}\n",
    "    })\n",
    "\n",
    "    # Define the models you want to train, including YOLOv10\n",
    "    models_to_train = [\n",
    "        'resnet50',\n",
    "        'efficientnet_b0',\n",
    "        'vit_base_patch16_224',\n",
    "        'deit_base_patch16_224',\n",
    "        'swin_base_patch4_window7_224',\n",
    "        'convnext_base',\n",
    "        'yolov10' \n",
    "    ]\n",
    "    \n",
    "    # Train and evaluate all models\n",
    "    results = clf.train_and_evaluate(train_loader, val_loader, test_loader, models_to_train)\n",
    "    \n",
    "    # Plot results\n",
    "    clf.plot_results()\n",
    "    \n",
    "    # Plot training history for each model\n",
    "    for model_name in clf.models.keys():\n",
    "        clf.plot_training_history(model_name)\n",
    "    \n",
    "    # Plot confusion matrix for the best model\n",
    "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "    clf.plot_confusion_matrix(best_model_name)\n",
    "    \n",
    "    # Print detailed results\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        for metric, value in result.items():\n",
    "            if metric != 'confusion_matrix':\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\nBest model: {best_model_name} with accuracy {clf.best_score:.4f}\")\n",
    "\n",
    "    # YOLOv10 specific data loaders and training\n",
    "    train_loader, val_loader, test_loader = clf.create_data_loaders(\n",
    "        {'train': {'image_paths': X_train, 'labels': y_train}, 'val': {'image_paths': X_val, 'labels': y_val}, 'test': {'image_paths': X_test, 'labels': y_test}},\n",
    "        model_type='yolo'\n",
    "    )\n",
    "    \n",
    "    # Train YOLOv10\n",
    "    model = clf.create_model('yolov10')\n",
    "    model, history = clf.train_model(model, train_loader, val_loader)\n",
    "\n",
    "    # Evaluate YOLOv10\n",
    "    results = clf.evaluate_model(model, test_loader)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
